{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dynamic_programming import MarkovDecisionProcess, Policy, calculate_value_function, calculate_action_value_function, calculate_action_value_function_from_policy, calculate_greedy_policy, policy_iteration, calculate_optimal_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function [ 1.90000000e+00  1.00000000e+00  2.77555756e-17 -1.38777878e-16]\n",
      "action value function [[ 1.9000000e+00]\n",
      " [ 1.0000000e+00]\n",
      " [-1.2490009e-16]\n",
      " [-1.2490009e-16]]\n",
      "greedy policy [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = np.array([\n",
    "    [\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 0, 0, 1]\n",
    "    ]\n",
    "])\n",
    "\n",
    "rewards = np.array([1, 1, 1, 0])\n",
    "\n",
    "action_probabilities = np.array([[1], [1], [1], [1]])\n",
    "\n",
    "mdp = MarkovDecisionProcess(transition_probabilities, rewards, 0.9)\n",
    "policy = Policy(action_probabilities)\n",
    "\n",
    "value_function = calculate_value_function(mdp, policy)\n",
    "print(\"value function\", value_function)\n",
    "action_value_function = calculate_action_value_function_from_policy(mdp, policy)\n",
    "print(\"action value function\", action_value_function)\n",
    "greedy_policy = calculate_greedy_policy(action_value_function)\n",
    "print(\"greedy policy\", greedy_policy.action_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions: 0 -> right, 1 -> up, 2 -> left, 3 -> down\n",
    "def make_grid_mdp(grid_size: int, discount: float):\n",
    "    n_states = grid_size * grid_size\n",
    "\n",
    "    transition_probabilities = np.zeros(shape=(4, n_states, n_states))\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            cell_index = x + grid_size * y\n",
    "            \n",
    "            if not ((x == 0 and y == 0) or (x == grid_size -1 and y == grid_size - 1)):\n",
    "                transition_probabilities[0, cell_index, cell_index + (x < grid_size - 1)] = 1\n",
    "                transition_probabilities[1, cell_index, cell_index - grid_size * (y > 0)] = 1\n",
    "                transition_probabilities[2, cell_index, cell_index - (x > 0)] = 1\n",
    "                transition_probabilities[3, cell_index, cell_index + grid_size * (y < grid_size - 1)] = 1\n",
    "            else:\n",
    "                transition_probabilities[:, cell_index, cell_index] = 1./4\n",
    "\n",
    "\n",
    "    rewards = np.full(shape=n_states, fill_value=-1)\n",
    "    rewards[0] = rewards[-1] = 0\n",
    "\n",
    "    return MarkovDecisionProcess(\n",
    "        transition_probabilities=transition_probabilities,\n",
    "        rewards=rewards,\n",
    "        discount=discount\n",
    "    )\n",
    "\n",
    "\n",
    "def make_uniform_grid_policy(grid_size: int):\n",
    "    return Policy(np.full(shape=(grid_size * grid_size, 4), fill_value=1.0/4))\n",
    "\n",
    "\n",
    "def print_grid_values(grid_size: int, values: np.ndarray):\n",
    "    values = values.reshape(grid_size, grid_size)\n",
    "\n",
    "    for row in values:\n",
    "        value_strs = (f\" {value:.1f} \".rjust(6) for value in row)\n",
    "        print(*value_strs, sep='|')\n",
    "\n",
    "# actions: 0 -> right, 1 -> up, 2 -> left, 3 -> down\n",
    "# arrows = ['→', '↑', '←', '↓']\n",
    "def print_grid_policy(grid_size: int, policy: Policy):\n",
    "    action_props = policy.action_probabilities.reshape(grid_size, grid_size, -1)\n",
    "\n",
    "    for y, row in enumerate(action_props):\n",
    "\n",
    "        for x, cell_props in enumerate(row):\n",
    "            print(\"    ↑    \" if cell_props[1] > 0 else \"         \", end='\\n' if x == grid_size - 1 else '|')\n",
    "\n",
    "        for x, cell_props in enumerate(row):\n",
    "            left_str = \"←\" if cell_props[2] > 0 else \" \"\n",
    "            right_str = \"→\" if cell_props[0] > 0 else \" \"\n",
    "\n",
    "            print(f\" {left_str}     {right_str} \", end='\\n' if x == grid_size - 1 else '|')\n",
    "\n",
    "        for x, cell_props in enumerate(row):\n",
    "            print(\"    ↓    \" if cell_props[3] > 0 else \"         \", end='\\n' if x == grid_size - 1 else '|')\n",
    "\n",
    "\n",
    "        if y < grid_size - 1:\n",
    "            print('-' * (10 * grid_size - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 6\n",
    "\n",
    "mdp = make_grid_mdp(grid_size, 0.9)\n",
    "policy = make_uniform_grid_policy(grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function uniform policy\n",
      " -0.0 | -5.3 | -7.8 | -8.9 | -9.3 | -9.4 \n",
      " -5.3 | -7.2 | -8.4 | -9.0 | -9.2 | -9.3 \n",
      " -7.8 | -8.4 | -8.8 | -9.0 | -9.0 | -8.9 \n",
      " -8.9 | -9.0 | -9.0 | -8.8 | -8.4 | -7.8 \n",
      " -9.3 | -9.2 | -9.0 | -8.4 | -7.2 | -5.3 \n",
      " -9.4 | -9.3 | -8.9 | -7.8 | -5.3 | -0.0 \n"
     ]
    }
   ],
   "source": [
    "value_function = calculate_value_function(mdp, policy)\n",
    "print(\"value function uniform policy\")\n",
    "print_grid_values(grid_size, value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action value function uniform policy for →\n",
      " -0.0 | -8.1 | -9.0 | -9.3 | -9.5 | -9.5 \n",
      " -7.5 | -8.5 | -9.1 | -9.3 | -9.3 | -9.3 \n",
      " -8.5 | -8.9 | -9.1 | -9.1 | -9.0 | -9.0 \n",
      " -9.1 | -9.1 | -8.9 | -8.5 | -8.1 | -8.1 \n",
      " -9.3 | -9.1 | -8.5 | -7.5 | -5.8 | -5.8 \n",
      " -9.3 | -9.0 | -8.1 | -5.8 | -0.0 | -0.0 \n"
     ]
    }
   ],
   "source": [
    "action_value_function = calculate_action_value_function(mdp, value_function)\n",
    "print(\"action value function uniform policy for →\")\n",
    "print_grid_values(grid_size, action_value_function[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy\n",
      "    ↑    |         |         |         |         |         \n",
      " ←     → | ←       | ←       | ←       | ←       | ←       \n",
      "    ↓    |         |         |         |         |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |         |         |         |         \n",
      "         | ←       | ←       | ←       | ←       |         \n",
      "         |         |         |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |         |         |         \n",
      "         |         | ←       | ←       |         |         \n",
      "         |         |         |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |         |         |         \n",
      "         |         |       → |       → |         |         \n",
      "         |         |         |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |         |         |         |         \n",
      "         |       → |       → |       → |       → |         \n",
      "         |         |         |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |         |         |         |    ↑    \n",
      "       → |       → |       → |       → |       → | ←     → \n",
      "         |         |         |         |         |    ↓    \n"
     ]
    }
   ],
   "source": [
    "greedy_policy = calculate_greedy_policy(action_value_function.round(2))\n",
    "print(\"optimal policy\")\n",
    "print_grid_policy(grid_size, greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function for optimal policy\n",
      "  0.0 |  0.0 | -1.0 | -1.9 | -2.7 | -3.4 \n",
      "  0.0 | -1.0 | -1.9 | -2.7 | -3.4 | -2.7 \n",
      " -1.0 | -1.9 | -2.7 | -3.4 | -2.7 | -1.9 \n",
      " -1.9 | -2.7 | -3.4 | -2.7 | -1.9 | -1.0 \n",
      " -2.7 | -3.4 | -2.7 | -1.9 | -1.0 |  0.0 \n",
      " -3.4 | -2.7 | -1.9 | -1.0 |  0.0 |  0.0 \n"
     ]
    }
   ],
   "source": [
    "value_function_for_optimal = calculate_value_function(mdp, greedy_policy)\n",
    "print(\"value function for optimal policy\")\n",
    "print_grid_values(grid_size, value_function_for_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy\n",
      "    ↑    |         |         |         |         |         \n",
      " ←     → | ←       | ←       | ←       | ←       | ←       \n",
      "    ↓    |         |         |         |         |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |    ↑    |    ↑    |         \n",
      "         | ←       | ←       | ←       | ←     → |         \n",
      "         |         |         |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |    ↑    |         |         \n",
      "         | ←       | ←       | ←     → |       → |         \n",
      "         |         |         |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |         |         |         \n",
      "         | ←       | ←     → |       → |       → |         \n",
      "         |         |    ↓    |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |         |         |         |         \n",
      "         | ←     → |       → |       → |       → |         \n",
      "         |    ↓    |    ↓    |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |         |         |         |    ↑    \n",
      "       → |       → |       → |       → |       → | ←     → \n",
      "         |         |         |         |         |    ↓    \n"
     ]
    }
   ],
   "source": [
    "greedy_policy2 = calculate_greedy_policy(calculate_action_value_function(mdp, value_function_for_optimal).round(2))\n",
    "print(\"optimal policy\")\n",
    "print_grid_policy(grid_size, greedy_policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy obtained from policy iteration algorithm\n",
      "    ↑    |         |         |         |         |         \n",
      " ←     → | ←       | ←       | ←       | ←       |         \n",
      "    ↓    |         |         |         |         |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |         |         |         |         \n",
      "         | ←       | ←       | ←       |         |         \n",
      "         |         |         |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |    ↑    |         |         |         \n",
      "         | ←       |         |       → |       → |         \n",
      "         |         |         |         |         |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |    ↑    |         |         |         \n",
      "         | ←       |         |       → |         |         \n",
      "         |         |    ↓    |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |         |         |         |         \n",
      "         | ←       |         |       → |         |         \n",
      "         |         |    ↓    |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |         |         |         |    ↑    \n",
      "         |       → |       → |       → |       → | ←     → \n",
      "         |         |         |         |         |    ↓    \n"
     ]
    }
   ],
   "source": [
    "optimal_policy = policy_iteration(mdp, make_uniform_grid_policy(grid_size), 10)\n",
    "print(\"policy obtained from policy iteration algorithm\")\n",
    "print_grid_policy(grid_size, optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal value function\n",
      "  0.0 |  0.0 | -1.0 | -1.9 | -2.7 | -3.4 \n",
      "  0.0 | -1.0 | -1.9 | -2.7 | -3.4 | -2.7 \n",
      " -1.0 | -1.9 | -2.7 | -3.4 | -2.7 | -1.9 \n",
      " -1.9 | -2.7 | -3.4 | -2.7 | -1.9 | -1.0 \n",
      " -2.7 | -3.4 | -2.7 | -1.9 | -1.0 |  0.0 \n",
      " -3.4 | -2.7 | -1.9 | -1.0 |  0.0 |  0.0 \n"
     ]
    }
   ],
   "source": [
    "optimal_value_function = calculate_optimal_value_function(mdp, 100)\n",
    "print(\"optimal value function\")\n",
    "print_grid_values(grid_size, optimal_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy policy associated to optimal value function\n",
      "    ↑    |         |         |         |         |         \n",
      " ←     → | ←       | ←       | ←       | ←       | ←       \n",
      "    ↓    |         |         |         |         |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |    ↑    |    ↑    |         \n",
      "         | ←       | ←       | ←       | ←     → |         \n",
      "         |         |         |         |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |    ↑    |         |         \n",
      "         | ←       | ←       | ←     → |       → |         \n",
      "         |         |         |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |    ↑    |         |         |         \n",
      "         | ←       | ←     → |       → |       → |         \n",
      "         |         |    ↓    |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |    ↑    |         |         |         |         \n",
      "         | ←     → |       → |       → |       → |         \n",
      "         |    ↓    |    ↓    |    ↓    |    ↓    |    ↓    \n",
      "-----------------------------------------------------------\n",
      "    ↑    |         |         |         |         |    ↑    \n",
      "       → |       → |       → |       → |       → | ←     → \n",
      "         |         |         |         |         |    ↓    \n"
     ]
    }
   ],
   "source": [
    "optimal_action_value_function = calculate_action_value_function(mdp, optimal_value_function)\n",
    "optimal_greedy_policy = calculate_greedy_policy(optimal_action_value_function)\n",
    "print(\"greedy policy associated to optimal value function\")\n",
    "print_grid_policy(grid_size, optimal_greedy_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
